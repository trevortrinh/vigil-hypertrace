{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Hyperliquid Data from S3\n",
    "\n",
    "Utilities for downloading data from Hyperliquid's S3 buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import msgpack\n",
    "import lz4.frame\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "AWS_PROFILE = 'trevor'\n",
    "OUTPUT_DIR = Path('../hyperliquid_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 Bucket Structure\n",
    "\n",
    "```\n",
    "s3://hl-mainnet-node-data/\n",
    "├── explorer_blocks/          # Raw blocks (Feb 2023+)\n",
    "├── node_trades/hourly/       # Parsed trades (Mar 2025+)\n",
    "├── node_fills/hourly/        # Fills + PnL (May 2025+)\n",
    "└── replica_cmds/             # Raw L1 commands (Jan 2025+)\n",
    "\n",
    "s3://hyperliquid-archive/\n",
    "├── market_data/              # L2 orderbook (Apr 2023+)\n",
    "└── asset_ctxs/               # Asset contexts\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_ls(path):\n",
    "    \"\"\"List S3 path contents\"\"\"\n",
    "    cmd = f'AWS_PROFILE={AWS_PROFILE} aws s3 ls \"{path}\" --request-payer requester'\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    return result.stdout.strip().split('\\n')\n",
    "\n",
    "def s3_cp(src, dst):\n",
    "    \"\"\"Copy file from S3\"\"\"\n",
    "    cmd = f'AWS_PROFILE={AWS_PROFILE} aws s3 cp \"{src}\" \"{dst}\" --request-payer requester'\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    return result.returncode == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Available Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List top-level prefixes in node-data bucket\n",
    "print(\"hl-mainnet-node-data contents:\")\n",
    "for line in s3_ls('s3://hl-mainnet-node-data/'):\n",
    "    print(f\"  {line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List explorer_blocks prefixes (block ranges)\n",
    "print(\"explorer_blocks prefixes:\")\n",
    "for line in s3_ls('s3://hl-mainnet-node-data/explorer_blocks/'):\n",
    "    print(f\"  {line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check node_fills date range\n",
    "print(\"node_fills earliest dates:\")\n",
    "for line in s3_ls('s3://hl-mainnet-node-data/node_fills/hourly/')[:5]:\n",
    "    print(f\"  {line}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Explorer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_explorer_block(block_num, output_dir=None):\n",
    "    \"\"\"\n",
    "    Download explorer block file containing the given block number.\n",
    "    Files are batched by 100 blocks.\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir or OUTPUT_DIR / 'explorer_blocks')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Calculate file path\n",
    "    # Files are named by ending block, batched by 100\n",
    "    file_block = ((block_num // 100) + 1) * 100\n",
    "    prefix_100m = (file_block // 100_000_000) * 100_000_000\n",
    "    prefix_100k = (file_block // 100_000) * 100_000\n",
    "    \n",
    "    s3_path = f's3://hl-mainnet-node-data/explorer_blocks/{prefix_100m}/{prefix_100k}/{file_block}.rmp.lz4'\n",
    "    local_path = output_dir / f'{file_block}.rmp.lz4'\n",
    "    \n",
    "    print(f\"Downloading {s3_path}...\")\n",
    "    if s3_cp(s3_path, str(local_path)):\n",
    "        print(f\"Saved to {local_path}\")\n",
    "        return local_path\n",
    "    else:\n",
    "        print(\"Download failed\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Download blocks around block 500,000,000\n",
    "# download_explorer_block(500_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Node Fills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_node_fills(date_str, hour=None, output_dir=None):\n",
    "    \"\"\"\n",
    "    Download node_fills for a specific date (YYYYMMDD) and optionally hour (00-23).\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir or OUTPUT_DIR / 'node_fills')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if hour is not None:\n",
    "        s3_path = f's3://hl-mainnet-node-data/node_fills/hourly/{date_str}/{hour:02d}/'\n",
    "    else:\n",
    "        s3_path = f's3://hl-mainnet-node-data/node_fills/hourly/{date_str}/'\n",
    "    \n",
    "    # List files\n",
    "    files = s3_ls(s3_path)\n",
    "    print(f\"Found {len(files)} files in {s3_path}\")\n",
    "    \n",
    "    for line in files[:5]:  # Show first 5\n",
    "        print(f\"  {line}\")\n",
    "    \n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Check fills for a recent date\n",
    "# download_node_fills('20251101', hour=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompress and Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompress_lz4(filepath):\n",
    "    \"\"\"Decompress LZ4 file\"\"\"\n",
    "    filepath = Path(filepath)\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return lz4.frame.decompress(f.read())\n",
    "\n",
    "def parse_msgpack(data):\n",
    "    \"\"\"Parse MessagePack data\"\"\"\n",
    "    return msgpack.unpackb(data, raw=False)\n",
    "\n",
    "def parse_jsonl(data):\n",
    "    \"\"\"Parse JSONL (newline-delimited JSON)\"\"\"\n",
    "    lines = data.decode().strip().split('\\n')\n",
    "    return [json.loads(line) for line in lines]\n",
    "\n",
    "def load_explorer_block(filepath):\n",
    "    \"\"\"Load explorer block from .rmp.lz4 file\"\"\"\n",
    "    data = decompress_lz4(filepath)\n",
    "    return parse_msgpack(data)\n",
    "\n",
    "def load_node_fills(filepath):\n",
    "    \"\"\"Load node_fills from .lz4 file\"\"\"\n",
    "    data = decompress_lz4(filepath)\n",
    "    return parse_jsonl(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# blocks = load_explorer_block('../hyperliquid_samples/explorer_blocks/811681900.rmp.lz4')\n",
    "# print(f\"Loaded {len(blocks)} blocks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
