{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Hyperliquid Data Sources\n",
    "\n",
    "Download samples from each S3 bucket and decompress to readable JSON/CSV.\n",
    "\n",
    "**Output**: Sample files in `../hyperliquid_samples/` that you can browse in your file explorer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples will be saved to: /Users/trevor/Developer/trevor/vigil-contract/hyperliquid_samples\n",
      "  hyperliquid-archive → hyperliquid-archive/\n",
      "  hl-mainnet-node-data → hl-mainnet-node-data/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import lz4.frame\n",
    "import msgpack\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# S3 client with requester-pays\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "    aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "    region_name=os.getenv('AWS_REGION', 'us-east-2')\n",
    ")\n",
    "\n",
    "REQUEST_PAYER = {'RequestPayer': 'requester'}\n",
    "\n",
    "# Output directories organized by bucket\n",
    "SAMPLES_DIR = Path('../hyperliquid_samples')\n",
    "ARCHIVE_DIR = SAMPLES_DIR / 'hyperliquid-archive'\n",
    "NODE_DATA_DIR = SAMPLES_DIR / 'hl-mainnet-node-data'\n",
    "\n",
    "SAMPLES_DIR.mkdir(exist_ok=True)\n",
    "ARCHIVE_DIR.mkdir(exist_ok=True)\n",
    "NODE_DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Samples will be saved to: {SAMPLES_DIR.resolve()}\")\n",
    "print(f\"  hyperliquid-archive → {ARCHIVE_DIR.name}/\")\n",
    "print(f\"  hl-mainnet-node-data → {NODE_DATA_DIR.name}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_prefixes(bucket, prefix=''):\n",
    "    \"\"\"List folder prefixes in S3\"\"\"\n",
    "    r = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, Delimiter='/', **REQUEST_PAYER)\n",
    "    return [p['Prefix'] for p in r.get('CommonPrefixes', [])]\n",
    "\n",
    "def list_files(bucket, prefix, limit=100):\n",
    "    \"\"\"List files in S3\"\"\"\n",
    "    r = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, MaxKeys=limit, **REQUEST_PAYER)\n",
    "    return [(obj['Key'], obj['Size']) for obj in r.get('Contents', [])]\n",
    "\n",
    "def download(bucket, key):\n",
    "    \"\"\"Download file from S3\"\"\"\n",
    "    return s3.get_object(Bucket=bucket, Key=key, **REQUEST_PAYER)['Body'].read()\n",
    "\n",
    "def decompress_lz4(data):\n",
    "    \"\"\"Decompress LZ4 data\"\"\"\n",
    "    return lz4.frame.decompress(data)\n",
    "\n",
    "def decode_msgpack(data):\n",
    "    \"\"\"Decode MessagePack data\"\"\"\n",
    "    return msgpack.unpackb(data, raw=False)\n",
    "\n",
    "def save_json(data, path):\n",
    "    \"\"\"Save data as formatted JSON\"\"\"\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f, indent=2, default=str)\n",
    "    print(f\"Saved: {path} ({path.stat().st_size / 1024:.1f} KB)\")\n",
    "\n",
    "def save_text(data, path):\n",
    "    \"\"\"Save raw text/CSV\"\"\"\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, 'wb') as f:\n",
    "        f.write(data)\n",
    "    print(f\"Saved: {path} ({path.stat().st_size / 1024:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. hyperliquid-archive\n",
    "\n",
    "Market data archives. Less relevant for trader analysis, but let's see what's there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperliquid-archive structure:\n",
      "  Testnet/\n",
      "  asset_ctxs/\n",
      "  market_data/\n"
     ]
    }
   ],
   "source": [
    "print(\"hyperliquid-archive structure:\")\n",
    "for p in list_prefixes('hyperliquid-archive'):\n",
    "    print(f\"  {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Asset Contexts (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available asset_ctxs files:\n",
      "  asset_ctxs/20230520.csv.lz4 (981.4 KB)\n",
      "  asset_ctxs/20230521.csv.lz4 (1137.4 KB)\n",
      "  asset_ctxs/20230522.csv.lz4 (1186.6 KB)\n",
      "  asset_ctxs/20230523.csv.lz4 (1173.3 KB)\n",
      "  asset_ctxs/20230524.csv.lz4 (1312.8 KB)\n"
     ]
    }
   ],
   "source": [
    "# Download asset context sample\n",
    "files = list_files('hyperliquid-archive', 'asset_ctxs/', limit=5)\n",
    "print(\"Available asset_ctxs files:\")\n",
    "for key, size in files:\n",
    "    print(f\"  {key} ({size/1024:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../hyperliquid_samples/hyperliquid-archive/asset_ctxs/20240101.csv (16072.7 KB)\n",
      "\n",
      "First 500 chars:\n",
      "time,coin,funding,open_interest,prev_day_px,day_ntl_vlm,premium,oracle_px,mark_px,mid_px,impact_bid_px,impact_ask_px\n",
      "2024-01-01T00:00:00Z,AAVE,0.00002789,1075.9,111.09,1759054.2941,0.00072316,108.69,108.79,108.765,108.75,108.7872\n",
      "2024-01-01T00:00:00Z,ACE,0.00004605,9238.23,10.331,1168558.484093,0.00086837,9.3278,9.331,9.3361,9.334,9.3378\n",
      "2024-01-01T00:00:00Z,ADA,0.0000125,725829,0.60135,723752.77572,0.00048152,0.59395,0.59424,0.594245,0.594144,0.594328\n",
      "2024-01-01T00:00:00Z,APE,0.00013221,136489.\n"
     ]
    }
   ],
   "source": [
    "# Download and decompress\n",
    "sample_key = 'asset_ctxs/20240101.csv.lz4'\n",
    "data = download('hyperliquid-archive', sample_key)\n",
    "decompressed = decompress_lz4(data)\n",
    "\n",
    "# Save as CSV\n",
    "save_text(decompressed, ARCHIVE_DIR / 'asset_ctxs' / '20240101.csv')\n",
    "\n",
    "# Preview\n",
    "print(\"\\nFirst 500 chars:\")\n",
    "print(decompressed[:500].decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. L2 Book (JSON lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Book files:\n",
      "  AAVE.lz4 (243.8 KB)\n",
      "  ACE.lz4 (491.5 KB)\n",
      "  ADA.lz4 (297.2 KB)\n",
      "  APE.lz4 (205.3 KB)\n",
      "  APT.lz4 (332.1 KB)\n"
     ]
    }
   ],
   "source": [
    "# Find available L2 book files\n",
    "files = list_files('hyperliquid-archive', 'market_data/20240101/12/l2Book/', limit=5)\n",
    "print(\"L2 Book files:\")\n",
    "for key, size in files:\n",
    "    print(f\"  {key.split('/')[-1]} ({size/1024:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../hyperliquid_samples/hyperliquid-archive/market_data/BTC_l2book_sample.json (462.8 KB)\n",
      "\n",
      "Sample L2 book record:\n",
      "{\n",
      "  \"time\": \"2024-01-01T12:00:01.138942031\",\n",
      "  \"ver_num\": 1,\n",
      "  \"raw\": {\n",
      "    \"channel\": \"l2Book\",\n",
      "    \"data\": {\n",
      "      \"coin\": \"BTC\",\n",
      "      \"time\": 1704110399915,\n",
      "      \"levels\": [\n",
      "        [\n",
      "          {\n",
      "            \"px\": \"42727.0\",\n",
      "            \"sz\": \"0.02265\",\n",
      "            \"n\": 1\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42723.0\",\n",
      "            \"sz\": \"0.17828\",\n",
      "            \"n\": 1\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42720.0\",\n",
      "            \"sz\": \"0.07722\",\n",
      "            \"n\": 1\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42719.0\",\n",
      "            \"sz\": \"0.0234\",\n",
      "            \"n\": 1\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42718.0\",\n",
      "            \"sz\": \"4.00898\",\n",
      "            \"n\": 2\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42711.0\",\n",
      "            \"sz\": \"7.65792\",\n",
      "            \"n\": 2\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42709.0\",\n",
      "            \"sz\": \"4.06794\",\n",
      "            \"n\": 1\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42708.0\",\n",
      "            \"sz\": \"0.04\",\n",
      "            \"n\": 1\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42705.0\",\n",
      "            \"sz\": \"3.83785\",\n",
      "            \"n\": 1\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42701.0\",\n",
      "            \"sz\": \"3.47576\",\n",
      "            \"n\": 1\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42686.0\",\n",
      "            \"sz\": \"3.46767\",\n",
      "            \"n\": 1\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42684.0\",\n",
      "            \"sz\": \"0.08785\",\n",
      "            \"n\": 1\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42683.0\",\n",
      "            \"sz\": \"0.0062\",\n",
      "            \"n\": 1\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42681.0\",\n",
      "            \"sz\": \"4.02076\",\n",
      "            \"n\": 1\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42674.0\",\n",
      "            \"sz\": \"3.90369\",\n",
      "            \"n\": 1\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42672.0\",\n",
      "            \"sz\": \"0.1173\",\n",
      "            \"n\": 1\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42666.0\",\n",
      "            \"sz\": \"0.0293\",\n",
      "            \"n\": 1\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42650.0\",\n",
      "            \"sz\": \"8.04079\",\n",
      "            \"n\": 2\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42627.0\",\n",
      "            \"sz\": \"8.14266\",\n",
      "            \"n\": 2\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42623.0\",\n",
      "            \"sz\": \"0.02933\",\n",
      "            \"n\": 1\n",
      "          }\n",
      "        ],\n",
      "        [\n",
      "          {\n",
      "            \"px\": \"42729.0\",\n",
      "            \"sz\": \"3.83257\",\n",
      "            \"n\": 2\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42731.0\",\n",
      "            \"sz\": \"0.397\",\n",
      "            \"n\": 2\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42734.0\",\n",
      "            \"sz\": \"3.77308\",\n",
      "            \"n\": 2\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42735.0\",\n",
      "            \"sz\": \"8.35117\",\n",
      "            \"n\": 2\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42739.0\",\n",
      "            \"sz\": \"7.82054\",\n",
      "            \"n\": 2\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42742.0\",\n",
      "            \"sz\": \"0.01683\",\n",
      "            \"n\": 2\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42745.0\",\n",
      "            \"sz\": \"8.26053\",\n",
      "            \"n\": 2\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42748.0\",\n",
      "            \"sz\": \"0.0234\",\n",
      "            \"n\": 1\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42750.0\",\n",
      "            \"sz\": \"7.96804\",\n",
      "            \"n\": 2\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42777.0\",\n",
      "            \"sz\": \"7.67932\",\n",
      "            \"n\": 2\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42785.0\",\n",
      "            \"sz\": \"0.11719\",\n",
      "            \"n\": 1\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42820.0\",\n",
      "            \"sz\": \"7.65512\",\n",
      "            \"n\": 2\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42868.0\",\n",
      "            \"sz\": \"0.08748\",\n",
      "            \"n\": 1\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42869.0\",\n",
      "            \"sz\": \"10.0\",\n",
      "            \"n\": 1\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42871.0\",\n",
      "            \"sz\": \"0.02916\",\n",
      "            \"n\": 1\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42891.0\",\n",
      "            \"sz\": \"7.86342\",\n",
      "            \"n\": 2\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42900.0\",\n",
      "            \"sz\": \"0.10097\",\n",
      "            \"n\": 3\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42914.0\",\n",
      "            \"sz\": \"0.02913\",\n",
      "            \"n\": 1\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"42950.0\",\n",
      "            \"sz\": \"2.5\",\n",
      "            \"n\": 1\n",
      "          },\n",
      "          {\n",
      "            \"px\": \"43000.0\",\n",
      "            \"sz\": \"0.00262\",\n",
      "            \"n\": 1\n",
      "          }\n",
      "        ]\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Download BTC L2 book sample\n",
    "sample_key = 'market_data/20240101/12/l2Book/BTC.lz4'\n",
    "data = download('hyperliquid-archive', sample_key)\n",
    "decompressed = decompress_lz4(data)\n",
    "\n",
    "# Parse JSON lines and save first 100 as JSON array\n",
    "lines = decompressed.decode().strip().split('\\n')\n",
    "sample = [json.loads(line) for line in lines[:100]]\n",
    "save_json(sample, ARCHIVE_DIR / 'market_data' / 'BTC_l2book_sample.json')\n",
    "\n",
    "# Preview first record\n",
    "print(\"\\nSample L2 book record:\")\n",
    "print(json.dumps(sample[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. hl-mainnet-node-data\n",
    "\n",
    "Node-streamed data. This is where the good stuff is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hl-mainnet-node-data structure:\n",
      "  explorer_blocks/\n",
      "  misc_events_by_block/\n",
      "  node_fills/\n",
      "  node_fills_by_block/\n",
      "  node_trades/\n",
      "  replica_cmds/\n",
      "\n",
      "============================================================\n",
      "Date Ranges:\n",
      "============================================================\n",
      "Fills by block (best)     Jul 27, 2025 → Nov 29, 2025 (126 days)\n",
      "Node fills (legacy)       May 25, 2025 → Jul 27, 2025 (64 days)\n",
      "Node trades (legacy)      Mar 22, 2025 → Jun 21, 2025 (66 days)\n",
      "Replica commands          No data found\n",
      "Misc events               Sep 27, 2025 → Nov 29, 2025 (64 days)\n",
      "Explorer blocks           Block 0 → 800000000+ (since Feb 2023)\n"
     ]
    }
   ],
   "source": [
    "print(\"hl-mainnet-node-data structure:\")\n",
    "for p in list_prefixes('hl-mainnet-node-data'):\n",
    "    print(f\"  {p}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Date Ranges:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def get_date_range(prefix):\n",
    "    \"\"\"Get first/last date for an hourly prefix\"\"\"\n",
    "    dates = list_prefixes('hl-mainnet-node-data', f'{prefix}hourly/')\n",
    "    if not dates:\n",
    "        return None, None, 0\n",
    "    first = dates[0].rstrip('/').split('/')[-1]\n",
    "    last = dates[-1].rstrip('/').split('/')[-1]\n",
    "    return first, last, len(dates)\n",
    "\n",
    "datasets = [\n",
    "    ('node_fills_by_block/', 'Fills by block (best)'),\n",
    "    ('node_fills/', 'Node fills (legacy)'),\n",
    "    ('node_trades/', 'Node trades (legacy)'),\n",
    "    ('replica_cmds/', 'Replica commands'),\n",
    "    ('misc_events_by_block/', 'Misc events'),\n",
    "]\n",
    "\n",
    "for prefix, label in datasets:\n",
    "    first, last, count = get_date_range(prefix)\n",
    "    if first:\n",
    "        first_fmt = datetime.strptime(first, '%Y%m%d').strftime('%b %d, %Y')\n",
    "        last_fmt = datetime.strptime(last, '%Y%m%d').strftime('%b %d, %Y')\n",
    "        print(f\"{label:25} {first_fmt} → {last_fmt} ({count} days)\")\n",
    "    else:\n",
    "        print(f\"{label:25} No data found\")\n",
    "\n",
    "# Explorer blocks are organized differently\n",
    "prefixes = list_prefixes('hl-mainnet-node-data', 'explorer_blocks/')\n",
    "if prefixes:\n",
    "    print(f\"{'Explorer blocks':25} Block 0 → {prefixes[-1].split('/')[1]}+ (since Feb 2023)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. node_fills_by_block (Best Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files for 20251128:\n",
      "  0.lz4 (16.0 MB)\n",
      "  1.lz4 (21.2 MB)\n",
      "  10.lz4 (18.6 MB)\n",
      "  11.lz4 (23.5 MB)\n",
      "  12.lz4 (19.5 MB)\n"
     ]
    }
   ],
   "source": [
    "# Get latest date and download one hour of fills\n",
    "dates = list_prefixes('hl-mainnet-node-data', 'node_fills_by_block/hourly/')\n",
    "latest_date = dates[-2].split('/')[-2]  # Second to last (likely complete)\n",
    "\n",
    "files = list_files('hl-mainnet-node-data', f'node_fills_by_block/hourly/{latest_date}/', limit=5)\n",
    "print(f\"Files for {latest_date}:\")\n",
    "for key, size in files:\n",
    "    print(f\"  {key.split('/')[-1]} ({size/1024/1024:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading node_fills_by_block/hourly/20251128/12.lz4...\n",
      "Loaded 44,437 blocks\n",
      "Extracted 230,074 fills\n",
      "Saved: ../hyperliquid_samples/hl-mainnet-node-data/node_fills_by_block/20251128_12_sample.json (569.5 KB)\n",
      "\n",
      "Sample fill:\n",
      "{\n",
      "  \"coin\": \"HYPE\",\n",
      "  \"px\": \"35.982\",\n",
      "  \"sz\": \"104.87\",\n",
      "  \"side\": \"B\",\n",
      "  \"time\": 1764331199926,\n",
      "  \"startPosition\": \"-4036.68\",\n",
      "  \"dir\": \"Close Short\",\n",
      "  \"closedPnl\": \"-77.089937\",\n",
      "  \"hash\": \"0x8535edac35dc9a1786af043059f8ba02031b0091d0dfb8e928fe98fef4d07402\",\n",
      "  \"oid\": 251510202190,\n",
      "  \"crossed\": true,\n",
      "  \"fee\": \"0.0\",\n",
      "  \"tid\": 124684432082590,\n",
      "  \"feeToken\": \"USDC\",\n",
      "  \"twapId\": null,\n",
      "  \"user\": \"0x010461c14e146ac35fe42271bdc1134ee31c703a\",\n",
      "  \"block_time\": \"2025-11-28T11:59:59.926782774\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Download hour 12 (midday, usually active)\n",
    "sample_key = f'node_fills_by_block/hourly/{latest_date}/12.lz4'\n",
    "print(f\"Downloading {sample_key}...\")\n",
    "data = download('hl-mainnet-node-data', sample_key)\n",
    "decompressed = decompress_lz4(data)\n",
    "\n",
    "# Each line is a block with multiple fill events\n",
    "lines = decompressed.decode().strip().split('\\n')\n",
    "blocks = [json.loads(line) for line in lines]\n",
    "print(f\"Loaded {len(blocks):,} blocks\")\n",
    "\n",
    "# Flatten to individual fills: each event is [user_address, fill_data]\n",
    "fills = []\n",
    "for block in blocks:\n",
    "    for user, fill_data in block.get('events', []):\n",
    "        fill_data['user'] = user\n",
    "        fill_data['block_time'] = block['block_time']\n",
    "        fills.append(fill_data)\n",
    "\n",
    "print(f\"Extracted {len(fills):,} fills\")\n",
    "\n",
    "# Save sample (first 1000 fills)\n",
    "save_json(fills[:1000], NODE_DATA_DIR / 'node_fills_by_block' / f'{latest_date}_12_sample.json')\n",
    "\n",
    "# Preview\n",
    "print(\"\\nSample fill:\")\n",
    "print(json.dumps(fills[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. explorer_blocks (Raw Blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent block files:\n",
      "  811600100.rmp.lz4 (5347.6 KB)\n",
      "  811600200.rmp.lz4 (2486.2 KB)\n",
      "  811600300.rmp.lz4 (1729.9 KB)\n",
      "  811600400.rmp.lz4 (2790.9 KB)\n",
      "  811600500.rmp.lz4 (3112.1 KB)\n"
     ]
    }
   ],
   "source": [
    "# Download recent blocks (from 800M range)\n",
    "block_files = list_files('hl-mainnet-node-data', 'explorer_blocks/800000000/811600000/', limit=5)\n",
    "print(\"Recent block files:\")\n",
    "for key, size in block_files:\n",
    "    print(f\"  {key.split('/')[-1]} ({size/1024:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading explorer_blocks/800000000/811600000/811600100.rmp.lz4...\n",
      "Loaded 100 blocks\n",
      "Saved: ../hyperliquid_samples/hl-mainnet-node-data/explorer_blocks/811600100.json (115703.2 KB)\n",
      "\n",
      "Block header:\n",
      "{\n",
      "  \"block_time\": \"2025-11-28T20:54:18.103847361\",\n",
      "  \"height\": 811600001,\n",
      "  \"hash\": \"0xd70c139e745457b7c70b4d3bf380b91fb7b1019ebd84499deed7d4a2cb819948\",\n",
      "  \"proposer\": \"0xb796a00b6e50c3dd46e43346c921fe8e146f4e06\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Download and decode a block file\n",
    "if block_files:\n",
    "    sample_key = block_files[0][0]\n",
    "    print(f\"Downloading {sample_key}...\")\n",
    "    data = download('hl-mainnet-node-data', sample_key)\n",
    "    \n",
    "    # Decompress and decode MessagePack\n",
    "    decompressed = decompress_lz4(data)\n",
    "    blocks = decode_msgpack(decompressed)\n",
    "    print(f\"Loaded {len(blocks)} blocks\")\n",
    "    \n",
    "    # Save as JSON\n",
    "    block_num = sample_key.split('/')[-1].replace('.rmp.lz4', '')\n",
    "    save_json(blocks, NODE_DATA_DIR / 'explorer_blocks' / f'{block_num}.json')\n",
    "    \n",
    "    # Preview\n",
    "    print(\"\\nBlock header:\")\n",
    "    print(json.dumps(blocks[0]['header'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. node_trades (Legacy Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Download node_trades sample (legacy format - ended June 2025)\ntrade_dates = list_prefixes('hl-mainnet-node-data', 'node_trades/hourly/')\n\nif trade_dates:\n    # Use middle of date range (edges may be incomplete)\n    sample_date = trade_dates[len(trade_dates)//2].split('/')[-2]\n    trade_files = list_files('hl-mainnet-node-data', f'node_trades/hourly/{sample_date}/', limit=24)\n    \n    # Find file with content\n    valid_file = next((key for key, size in trade_files if size > 100), None)\n    \n    if valid_file:\n        print(f\"Downloading {valid_file}...\")\n        data = download('hl-mainnet-node-data', valid_file)\n        content = decompress_lz4(data).decode()\n        lines = [l for l in content.strip().split('\\n') if l.strip()]\n        \n        trades = [json.loads(l) for l in lines[:100]]\n        save_json(trades, NODE_DATA_DIR / 'node_trades' / f'{sample_date}_sample.json')\n        \n        print(f\"\\nSample trade:\")\n        print(json.dumps(trades[0], indent=2))\nelse:\n    print(\"No node_trades data found\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Files saved to `../hyperliquid_samples/`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded samples:\n",
      "============================================================\n",
      "  README.md  (1.2 KB)\n",
      "  hl-mainnet-node-data/explorer_blocks/811600100.json  (115703.2 KB)\n",
      "  hl-mainnet-node-data/node_fills_by_block/20251128_12_sample.json  (569.5 KB)\n",
      "  hyperliquid-archive/asset_ctxs/20240101.csv  (16072.7 KB)\n",
      "  hyperliquid-archive/market_data/BTC_l2book_sample.json  (462.8 KB)\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloaded samples:\")\n",
    "print(\"=\" * 60)\n",
    "for path in sorted(SAMPLES_DIR.rglob('*')):\n",
    "    if path.is_file():\n",
    "        size_kb = path.stat().st_size / 1024\n",
    "        rel_path = path.relative_to(SAMPLES_DIR)\n",
    "        print(f\"  {rel_path}  ({size_kb:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which Format to Use?\n",
    "\n",
    "| Format | Best For | Complexity |\n",
    "|--------|----------|------------|\n",
    "| `node_fills_by_block` | **Trader analysis** | Low - ready to use |\n",
    "| `explorer_blocks` | Full history reconstruction | High - needs matching engine |\n",
    "| `node_trades` | Legacy trade data | Medium - less complete |\n",
    "| `asset_ctxs` | Market context | Low - CSV format |\n",
    "\n",
    "**Recommendation**: Use `node_fills_by_block` for all trader analysis. It has complete data since July 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you have sample data, proceed to **[02_analysis_pipeline.ipynb](./02_analysis_pipeline.ipynb)** to analyze it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}