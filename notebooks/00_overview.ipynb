{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperliquid Data Overview\n",
        "\n",
        "This notebook provides a complete overview of Hyperliquid's publicly available data infrastructure.\n",
        "\n",
        "**Goal**: Build a trading engine that deeply understands Hyperliquid through its traders.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Create `.env` from `.env.example`:\n",
        "```\n",
        "AWS_ACCESS_KEY_ID=...\n",
        "AWS_SECRET_ACCESS_KEY=...\n",
        "AWS_REGION=us-east-2\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ready\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import boto3\n",
        "import lz4.frame\n",
        "import msgpack\n",
        "from collections import Counter\n",
        "from dotenv import load_dotenv\n",
        "from datetime import datetime\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "s3 = boto3.client(\n",
        "    's3',\n",
        "    aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
        "    aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
        "    region_name=os.getenv('AWS_REGION', 'us-east-2')\n",
        ")\n",
        "\n",
        "REQUEST_PAYER = {'RequestPayer': 'requester'}\n",
        "\n",
        "def list_prefixes(bucket, prefix=''):\n",
        "    \"\"\"List folder prefixes in S3\"\"\"\n",
        "    r = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, Delimiter='/', **REQUEST_PAYER)\n",
        "    return [p['Prefix'] for p in r.get('CommonPrefixes', [])]\n",
        "\n",
        "def list_files(bucket, prefix, limit=100):\n",
        "    \"\"\"List files in S3\"\"\"\n",
        "    r = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, MaxKeys=limit, **REQUEST_PAYER)\n",
        "    return [obj['Key'] for obj in r.get('Contents', [])]\n",
        "\n",
        "def download(bucket, key):\n",
        "    \"\"\"Download file from S3\"\"\"\n",
        "    return s3.get_object(Bucket=bucket, Key=key, **REQUEST_PAYER)['Body'].read()\n",
        "\n",
        "def parse_jsonl_lz4(data):\n",
        "    \"\"\"Parse LZ4-compressed JSON lines\"\"\"\n",
        "    for line in lz4.frame.decompress(data).decode().strip().split('\\n'):\n",
        "        if line:\n",
        "            yield json.loads(line)\n",
        "\n",
        "def parse_msgpack_lz4(data):\n",
        "    \"\"\"Parse LZ4-compressed MessagePack\"\"\"\n",
        "    return msgpack.unpackb(lz4.frame.decompress(data), raw=False)\n",
        "\n",
        "print(\"Ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## The Two S3 Buckets\n",
        "\n",
        "From [Hyperliquid docs](https://hyperliquid.gitbook.io/hyperliquid-docs/historical-data):\n",
        "\n",
        "| Bucket | Purpose | Data Types |\n",
        "|--------|---------|------------|\n",
        "| `hyperliquid-archive` | Market data archives | Market data |\n",
        "| `hl-mainnet-node-data` | Node-streamed data | Explorer blocks, trades, fills |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== hyperliquid-archive ===\n",
            "  Testnet/\n",
            "  asset_ctxs/\n",
            "  market_data/\n",
            "\n",
            "=== hl-mainnet-node-data ===\n",
            "  explorer_blocks/\n",
            "  misc_events_by_block/\n",
            "  node_fills/\n",
            "  node_fills_by_block/\n",
            "  node_trades/\n",
            "  replica_cmds/\n"
          ]
        }
      ],
      "source": [
        "print(\"=== hyperliquid-archive ===\")\n",
        "for p in list_prefixes('hyperliquid-archive'):\n",
        "    print(f\"  {p}\")\n",
        "\n",
        "print(\"\\n=== hl-mainnet-node-data ===\")\n",
        "for p in list_prefixes('hl-mainnet-node-data'):\n",
        "    print(f\"  {p}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Bucket 1: `hyperliquid-archive`\n",
        "\n",
        "Market data archives. Not super important for our purposes.\n",
        "\n",
        "```\n",
        "s3://hyperliquid-archive/\n",
        "├── market_data/{YYYYMMDD}/{HH}/l2Book/{COIN}.lz4\n",
        "├── asset_ctxs/{YYYYMMDD}.csv.lz4\n",
        "└── Testnet/\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "market_data available: 20230415 to 20251102 (925 days)\n"
          ]
        }
      ],
      "source": [
        "# Discover actual date range from S3\n",
        "dates = list_prefixes('hyperliquid-archive', 'market_data/')\n",
        "first = dates[0].split('/')[-2]\n",
        "last = dates[-1].split('/')[-2]\n",
        "print(f\"market_data available: {first} to {last} ({len(dates)} days)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Bucket 2: `hl-mainnet-node-data`\n",
        "\n",
        "Node-streamed data. This is what we need for trader analysis.\n",
        "\n",
        "```\n",
        "s3://hl-mainnet-node-data/\n",
        "├── explorer_blocks/   \n",
        "├── node_trades/hourly/\n",
        "├── node_fills/hourly/\n",
        "├── node_fills_by_block/\n",
        "├── replica_cmds/\n",
        "└── misc_events_by_block/\n",
        "```\n",
        "\n",
        "| Dataset | First Date | Content | Format |\n",
        "|---------|------------|---------|--------|\n",
        "| `explorer_blocks` | Feb 2023 | Raw blocks: Orders, cancels (no fills) | MessagePack+LZ4 |\n",
        "| `node_trades` | Mar 2025 | Trades with buyer/seller | JSON+LZ4 |\n",
        "| `node_fills` | May 2025 | Fills with PnL, fees | JSON+LZ4 |\n",
        "| `node_fills_by_block` | Jul 2025 | Fills organized by block | JSON+LZ4 |\n",
        "| `replica_cmds` | Jul 2025 | L1 transactions | JSON+LZ4 |\n",
        "| `misc_events_by_block` | Jul 2025 | Liquidations, etc. | JSON+LZ4 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Date ranges discovered from S3:\n",
            "\n",
            "node_fills_by_block       July 27, 2025 to November 28, 2025 (125 days)\n",
            "node_fills                May 25, 2025 to July 27, 2025 (64 days)\n",
            "node_trades               March 22, 2025 to June 21, 2025 (66 days)\n"
          ]
        }
      ],
      "source": [
        "# Discover actual date ranges from S3\n",
        "def get_date_range(bucket, prefix):\n",
        "    dates = list_prefixes(bucket, prefix)\n",
        "    if not dates:\n",
        "        return None, None, 0\n",
        "    first = dates[0].rstrip('/').split('/')[-1]\n",
        "    last = dates[-1].rstrip('/').split('/')[-1]\n",
        "    return first, last, len(dates)\n",
        "\n",
        "datasets = [\n",
        "    ('node_fills_by_block', 'node_fills_by_block/hourly/'),\n",
        "    ('node_fills', 'node_fills/hourly/'),\n",
        "    ('node_trades', 'node_trades/hourly/'),\n",
        "]\n",
        "\n",
        "print(\"Date ranges discovered from S3:\\n\")\n",
        "for name, prefix in datasets:\n",
        "    first, last, count = get_date_range('hl-mainnet-node-data', prefix)\n",
        "    if first:\n",
        "        print(f\"{name:25} {datetime.strptime(first, '%Y%m%d').strftime('%B %d, %Y')} to {datetime.strptime(last, '%Y%m%d').strftime('%B %d, %Y')} ({count} days)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "explorer_blocks prefixes: ['0', '100000000', '200000000', '300000000', '400000000', '500000000', '600000000', '700000000', '800000000']\n",
            "\n",
            "Block ranges: 0 to 800000000+ (100M blocks each)\n"
          ]
        }
      ],
      "source": [
        "# Check explorer_blocks - different structure\n",
        "prefixes = list_prefixes('hl-mainnet-node-data', 'explorer_blocks/')\n",
        "print(f\"explorer_blocks prefixes: {[p.split('/')[1] for p in prefixes]}\")\n",
        "print(f\"\\nBlock ranges: 0 to {prefixes[-1].split('/')[1]}+ (100M blocks each)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exploring `node_fills_by_block` (The Best Data)\n",
        "\n",
        "Every fill since July 2025 with complete information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using date: 20251127\n",
            "Files in hour 12: 0\n"
          ]
        }
      ],
      "source": [
        "# Find a date with data\n",
        "dates = list_prefixes('hl-mainnet-node-data', 'node_fills_by_block/hourly/')\n",
        "sample_date = dates[-2].rstrip('/').split('/')[-1]  # Second to last (likely complete)\n",
        "print(f\"Using date: {sample_date}\")\n",
        "\n",
        "# List files for hour 12\n",
        "files = list_files('hl-mainnet-node-data', f'node_fills_by_block/hourly/{sample_date}/12/')\n",
        "print(f\"Files in hour 12: {len(files)}\")\n",
        "if files:\n",
        "    print(f\"First file: {files[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download and parse a sample\n",
        "if files:\n",
        "    data = download('hl-mainnet-node-data', files[0])\n",
        "    fills = list(parse_jsonl_lz4(data))\n",
        "    print(f\"Loaded {len(fills)} fills\\n\")\n",
        "    print(\"Sample fill:\")\n",
        "    print(json.dumps(fills[0], indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fill Schema\n",
        "\n",
        "| Field | Description |\n",
        "|-------|-------------|\n",
        "| `user` | Wallet address |\n",
        "| `coin` | Asset traded |\n",
        "| `px`, `sz` | Price and size |\n",
        "| `dir` | Direction: Open Long, Open Short, Close Long, Close Short |\n",
        "| `closedPnl` | Realized PnL (only on closes) |\n",
        "| `fee` | Fee paid |\n",
        "| `crossed` | true = taker, false = maker |\n",
        "| `startPosition` | Position before this fill |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick stats\n",
        "if files:\n",
        "    print(f\"Unique users: {len(set(f['user'] for f in fills))}\")\n",
        "    print(f\"Unique coins: {len(set(f['coin'] for f in fills))}\")\n",
        "    print(f\"\\nDirections:\")\n",
        "    for d, c in Counter(f['dir'] for f in fills).most_common():\n",
        "        print(f\"  {d}: {c}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exploring `explorer_blocks` (Raw Historical Data)\n",
        "\n",
        "For Feb 2023 - Jul 2025, only raw blocks exist. Must reconstruct fills."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early block files:\n",
            "  explorer_blocks/0/0/1000.rmp.lz4\n",
            "  explorer_blocks/0/0/10000.rmp.lz4\n",
            "  explorer_blocks/0/0/100000.rmp.lz4\n",
            "  explorer_blocks/0/0/10100.rmp.lz4\n",
            "  explorer_blocks/0/0/10200.rmp.lz4\n"
          ]
        }
      ],
      "source": [
        "# Get earliest block file\n",
        "block_files = list_files('hl-mainnet-node-data', 'explorer_blocks/0/0/', limit=5)\n",
        "print(\"Early block files:\")\n",
        "for f in block_files:\n",
        "    print(f\"  {f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 100 blocks\n",
            "Block range: 901 - 1000\n",
            "Time: 2023-02-26T17:41:39.942659\n"
          ]
        }
      ],
      "source": [
        "# Download and parse\n",
        "if block_files:\n",
        "    data = download('hl-mainnet-node-data', block_files[0])\n",
        "    blocks = parse_msgpack_lz4(data)\n",
        "    print(f\"Loaded {len(blocks)} blocks\")\n",
        "    print(f\"Block range: {blocks[0]['header']['height']} - {blocks[-1]['header']['height']}\")\n",
        "    print(f\"Time: {blocks[0]['header']['block_time']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action types:\n",
            "  order: 100\n",
            "  cancel: 25\n",
            "  SetGlobalAction: 5\n",
            "  CreditBridgeDepositAction: 1\n",
            "  connect: 1\n"
          ]
        }
      ],
      "source": [
        "# Action types in blocks\n",
        "if block_files:\n",
        "    actions = Counter()\n",
        "    for block in blocks:\n",
        "        for tx in block.get('txs', []):\n",
        "            for action in tx.get('actions', []):\n",
        "                actions[action.get('type', 'unknown')] += 1\n",
        "    \n",
        "    print(\"Action types:\")\n",
        "    for t, c in actions.most_common():\n",
        "        print(f\"  {t}: {c}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What's in Raw Blocks\n",
        "\n",
        "- Orders (asset, side, price, size)\n",
        "- Cancels\n",
        "- User addresses\n",
        "\n",
        "### What's Missing\n",
        "\n",
        "- Fills (must reconstruct via matching engine)\n",
        "- PnL (must calculate)\n",
        "- Fees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Reconstruction: Theoretical\n",
        "\n",
        "Fills can be reconstructed because matching is deterministic:\n",
        "\n",
        "```\n",
        "Same orders + Same sequence = Same fills\n",
        "```\n",
        "\n",
        "**Challenges:**\n",
        "- ~3-4 TB of data\n",
        "- Must track order book state\n",
        "- Edge cases: liquidations, funding, trigger orders\n",
        "\n",
        "**Effort estimate:**\n",
        "\n",
        "| Approach | Coverage | Time |\n",
        "|----------|----------|------|\n",
        "| `node_fills_by_block` only | Jul 2025+ (100%) | 1 day |\n",
        "| + API backfill (10k limit) | ~95% of fills | 3 days |\n",
        "| + Full reconstruction | 100% | 3-4 weeks |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Our Approach\n",
        "\n",
        "**Use `node_fills_by_block`** (Jul 2025 - Present)\n",
        "\n",
        "- ~5 months of complete fill data\n",
        "- Every trader, every fill, with PnL and fees\n",
        "- No reconstruction needed\n",
        "\n",
        "**What we can compute:**\n",
        "\n",
        "| Metric | Source |\n",
        "|--------|--------|\n",
        "| Realized PnL | `SUM(closedPnl)` |\n",
        "| Volume | `SUM(px * sz)` |\n",
        "| Trade count | `COUNT(*)` |\n",
        "| Maker % | `SUM(crossed=false) / COUNT(*)` |\n",
        "| Win rate | Closes with positive PnL / total closes |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Cost Estimates\n",
        "\n",
        "S3 transfer at $0.09/GB:\n",
        "\n",
        "| Dataset | Size (est.) | Cost |\n",
        "|---------|-------------|------|\n",
        "| `node_fills_by_block` | ~200-400 GB | ~$30 |\n",
        "| `explorer_blocks` | ~3-4 TB | ~$315 |\n",
        "\n",
        "**Start with `node_fills_by_block`.**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
