{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperliquid Data Overview\n",
    "\n",
    "This notebook provides a complete overview of Hyperliquid's publicly available data infrastructure.\n",
    "\n",
    "**Goal**: Build a trading engine that deeply understands Hyperliquid through its traders.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Create `.env` from `.env.example`:\n",
    "```\n",
    "AWS_ACCESS_KEY_ID=...\n",
    "AWS_SECRET_ACCESS_KEY=...\n",
    "AWS_REGION=us-east-2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import lz4.frame\n",
    "import msgpack\n",
    "from collections import Counter\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "    aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "    region_name=os.getenv('AWS_REGION', 'us-east-2')\n",
    ")\n",
    "\n",
    "REQUEST_PAYER = {'RequestPayer': 'requester'}\n",
    "\n",
    "def list_prefixes(bucket, prefix=''):\n",
    "    \"\"\"List folder prefixes in S3\"\"\"\n",
    "    r = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, Delimiter='/', **REQUEST_PAYER)\n",
    "    return [p['Prefix'] for p in r.get('CommonPrefixes', [])]\n",
    "\n",
    "def list_files(bucket, prefix, limit=100):\n",
    "    \"\"\"List files in S3\"\"\"\n",
    "    r = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, MaxKeys=limit, **REQUEST_PAYER)\n",
    "    return [obj['Key'] for obj in r.get('Contents', [])]\n",
    "\n",
    "def download(bucket, key):\n",
    "    \"\"\"Download file from S3\"\"\"\n",
    "    return s3.get_object(Bucket=bucket, Key=key, **REQUEST_PAYER)['Body'].read()\n",
    "\n",
    "def parse_jsonl_lz4(data):\n",
    "    \"\"\"Parse LZ4-compressed JSON lines\"\"\"\n",
    "    for line in lz4.frame.decompress(data).decode().strip().split('\\n'):\n",
    "        if line:\n",
    "            yield json.loads(line)\n",
    "\n",
    "def parse_msgpack_lz4(data):\n",
    "    \"\"\"Parse LZ4-compressed MessagePack\"\"\"\n",
    "    return msgpack.unpackb(lz4.frame.decompress(data), raw=False)\n",
    "\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Two S3 Buckets\n",
    "\n",
    "From [Hyperliquid docs](https://hyperliquid.gitbook.io/hyperliquid-docs/historical-data):\n",
    "\n",
    "| Bucket | Purpose | Data Types |\n",
    "|--------|---------|------------|\n",
    "| `hyperliquid-archive` | Market data archives | Market data |\n",
    "| `hl-mainnet-node-data` | Node-streamed data | Explorer blocks, trades, fills |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hyperliquid-archive ===\n",
      "  Testnet/\n",
      "  asset_ctxs/\n",
      "  market_data/\n",
      "\n",
      "=== hl-mainnet-node-data ===\n",
      "  explorer_blocks/\n",
      "  misc_events_by_block/\n",
      "  node_fills/\n",
      "  node_fills_by_block/\n",
      "  node_trades/\n",
      "  replica_cmds/\n"
     ]
    }
   ],
   "source": [
    "print(\"=== hyperliquid-archive ===\")\n",
    "for p in list_prefixes('hyperliquid-archive'):\n",
    "    print(f\"  {p}\")\n",
    "\n",
    "print(\"\\n=== hl-mainnet-node-data ===\")\n",
    "for p in list_prefixes('hl-mainnet-node-data'):\n",
    "    print(f\"  {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bucket 1: `hyperliquid-archive`\n",
    "\n",
    "Market data archives. Not the focus for trader analysis, but useful for orderbook studies.\n",
    "\n",
    "| Dataset | Date Range | Content | Format |\n",
    "|---------|------------|---------|--------|\n",
    "| `market_data` | Apr 2023 - Present | L2 orderbook snapshots per coin/hour | JSON+LZ4 |\n",
    "| `asset_ctxs` | May 2023 - Present | Daily asset context (funding, OI, etc.) | CSV+LZ4 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Dataset              First           Last              Days\n",
      "======================================================================\n",
      "market_data          Apr 15, 2023    Nov 02, 2025       925\n",
      "asset_ctxs           May 20, 2023    Nov 02, 2025       898\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Discover date ranges for hyperliquid-archive\n",
    "def fmt_date(d):\n",
    "    return datetime.strptime(d, '%Y%m%d').strftime('%b %d, %Y')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Dataset':<20} {'First':<15} {'Last':<15} {'Days':>6}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# market_data (organized by date folders)\n",
    "dates = list_prefixes('hyperliquid-archive', 'market_data/')\n",
    "if dates:\n",
    "    first = dates[0].split('/')[-2]\n",
    "    last = dates[-1].split('/')[-2]\n",
    "    print(f\"{'market_data':<20} {fmt_date(first):<15} {fmt_date(last):<15} {len(dates):>6}\")\n",
    "\n",
    "# asset_ctxs (files named by date)\n",
    "files = list_files('hyperliquid-archive', 'asset_ctxs/', limit=1000)\n",
    "if files:\n",
    "    # Extract dates from filenames like \"asset_ctxs/20230520.csv.lz4\"\n",
    "    file_dates = [f.split('/')[-1].split('.')[0] for f in files]\n",
    "    first = file_dates[0]\n",
    "    last = file_dates[-1]\n",
    "    print(f\"{'asset_ctxs':<20} {fmt_date(first):<15} {fmt_date(last):<15} {len(files):>6}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bucket 2: `hl-mainnet-node-data`\n",
    "\n",
    "Node-streamed data. This is what we need for trader analysis.\n",
    "\n",
    "| Dataset | Date Range | Content | Format |\n",
    "|---------|------------|---------|--------|\n",
    "| `node_fills_by_block` | **Jul 2025 - Present** | Fills with PnL, fees, maker/taker | JSON+LZ4 |\n",
    "| `node_fills` | May 2025 - Jul 2025 | Fills (legacy format) | JSON+LZ4 |\n",
    "| `node_trades` | Mar 2025 - Jun 2025 | Trades with buyer/seller | JSON+LZ4 |\n",
    "| `replica_cmds` | Jul 2025 - Present | L1 transactions | JSON+LZ4 |\n",
    "| `misc_events_by_block` | Jul 2025 - Present | Liquidations, funding, etc. | JSON+LZ4 |\n",
    "| `explorer_blocks` | Feb 2023 - Present | Raw blocks (orders, cancels) | MessagePack+LZ4 |\n",
    "\n",
    "**Best dataset**: `node_fills_by_block` â€” complete fill data, no reconstruction needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Dataset                   First           Last              Days\n",
      "======================================================================\n",
      "Fills by block (BEST)     Jul 27, 2025    Nov 29, 2025       126\n",
      "Node fills (legacy)       May 25, 2025    Jul 27, 2025        64\n",
      "Node trades (legacy)      Mar 22, 2025    Jun 21, 2025        66\n",
      "Replica commands          N/A             N/A                  0\n",
      "Misc events               Sep 27, 2025    Nov 29, 2025        64\n",
      "Explorer blocks           Block 0         Block 800000000+    N/A\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Discover all date ranges from S3\n",
    "def get_date_range(prefix):\n",
    "    \"\"\"Get first/last date for an hourly prefix\"\"\"\n",
    "    dates = list_prefixes('hl-mainnet-node-data', f'{prefix}hourly/')\n",
    "    if not dates:\n",
    "        return None, None, 0\n",
    "    first = dates[0].rstrip('/').split('/')[-1]\n",
    "    last = dates[-1].rstrip('/').split('/')[-1]\n",
    "    return first, last, len(dates)\n",
    "\n",
    "def fmt_date(d):\n",
    "    return datetime.strptime(d, '%Y%m%d').strftime('%b %d, %Y')\n",
    "\n",
    "datasets = [\n",
    "    ('node_fills_by_block/', 'Fills by block (BEST)'),\n",
    "    ('node_fills/', 'Node fills (legacy)'),\n",
    "    ('node_trades/', 'Node trades (legacy)'),\n",
    "    ('replica_cmds/', 'Replica commands'),\n",
    "    ('misc_events_by_block/', 'Misc events'),\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Dataset':<25} {'First':<15} {'Last':<15} {'Days':>6}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for prefix, label in datasets:\n",
    "    first, last, count = get_date_range(prefix)\n",
    "    if first:\n",
    "        print(f\"{label:<25} {fmt_date(first):<15} {fmt_date(last):<15} {count:>6}\")\n",
    "    else:\n",
    "        print(f\"{label:<25} {'N/A':<15} {'N/A':<15} {0:>6}\")\n",
    "\n",
    "# Explorer blocks - different structure (by block number, not date)\n",
    "prefixes = list_prefixes('hl-mainnet-node-data', 'explorer_blocks/')\n",
    "if prefixes:\n",
    "    last_range = prefixes[-1].split('/')[1]\n",
    "    print(f\"{'Explorer blocks':<25} {'Block 0':<15} {f'Block {last_range}+':<15} {'N/A':>6}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exploring `node_fills_by_block` (The Best Data)\n",
    "\n",
    "Every fill since July 2025 with complete information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using date: 20251128\n",
      "Files for this date: 24\n",
      "Sample files: ['0.lz4', '1.lz4', '10.lz4', '11.lz4', '12.lz4']\n"
     ]
    }
   ],
   "source": [
    "# Find a date with data\n",
    "dates = list_prefixes('hl-mainnet-node-data', 'node_fills_by_block/hourly/')\n",
    "sample_date = dates[-2].rstrip('/').split('/')[-1]  # Second to last (likely complete)\n",
    "print(f\"Using date: {sample_date}\")\n",
    "\n",
    "# List files for this date (files are named {hour}.lz4, not in hour subfolders)\n",
    "files = list_files('hl-mainnet-node-data', f'node_fills_by_block/hourly/{sample_date}/')\n",
    "print(f\"Files for this date: {len(files)}\")\n",
    "print(\"Sample files:\", [f.split('/')[-1] for f in files[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 44,437 blocks from hour 12\n",
      "Extracted 230,074 fills\n",
      "\n",
      "Sample fill:\n",
      "{\n",
      "  \"coin\": \"HYPE\",\n",
      "  \"px\": \"35.982\",\n",
      "  \"sz\": \"104.87\",\n",
      "  \"side\": \"B\",\n",
      "  \"time\": 1764331199926,\n",
      "  \"startPosition\": \"-4036.68\",\n",
      "  \"dir\": \"Close Short\",\n",
      "  \"closedPnl\": \"-77.089937\",\n",
      "  \"hash\": \"0x8535edac35dc9a1786af043059f8ba02031b0091d0dfb8e928fe98fef4d07402\",\n",
      "  \"oid\": 251510202190,\n",
      "  \"crossed\": true,\n",
      "  \"fee\": \"0.0\",\n",
      "  \"tid\": 124684432082590,\n",
      "  \"feeToken\": \"USDC\",\n",
      "  \"twapId\": null,\n",
      "  \"user\": \"0x010461c14e146ac35fe42271bdc1134ee31c703a\",\n",
      "  \"block_time\": \"2025-11-28T11:59:59.926782774\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Download and parse hour 12 (midday, typically active)\n",
    "sample_key = f'node_fills_by_block/hourly/{sample_date}/12.lz4'\n",
    "data = download('hl-mainnet-node-data', sample_key)\n",
    "\n",
    "# Each line is a block with multiple fill events\n",
    "blocks = list(parse_jsonl_lz4(data))\n",
    "print(f\"Loaded {len(blocks):,} blocks from hour 12\")\n",
    "\n",
    "# Flatten to individual fills: each event is [user_address, fill_data]\n",
    "fills = []\n",
    "for block in blocks:\n",
    "    for user, fill_data in block.get('events', []):\n",
    "        fill_data['user'] = user\n",
    "        fill_data['block_time'] = block['block_time']\n",
    "        fills.append(fill_data)\n",
    "\n",
    "print(f\"Extracted {len(fills):,} fills\\n\")\n",
    "print(\"Sample fill:\")\n",
    "print(json.dumps(fills[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structure\n",
    "\n",
    "Each file contains **blocks** (one per line), each block contains multiple **fill events**:\n",
    "\n",
    "```\n",
    "{block_time, block_number, events: [[user_address, fill_data], ...]}\n",
    "```\n",
    "\n",
    "### Fill Schema\n",
    "\n",
    "| Field | Description |\n",
    "|-------|-------------|\n",
    "| `user` | Wallet address (from event tuple) |\n",
    "| `coin` | Asset traded |\n",
    "| `px`, `sz` | Price and size |\n",
    "| `side` | B (buy) or A (ask/sell) |\n",
    "| `dir` | Direction: Open Long, Open Short, Close Long, Close Short, Long > Short, Short > Long |\n",
    "| `closedPnl` | Realized PnL (only on closes) |\n",
    "| `fee` | Fee paid (negative = rebate) |\n",
    "| `crossed` | true = taker, false = maker |\n",
    "| `startPosition` | Position before this fill |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique users: 5497\n",
      "Unique coins: 249\n",
      "\n",
      "Directions:\n",
      "  Open Short: 51788\n",
      "  Open Long: 47976\n",
      "  Close Short: 47480\n",
      "  Close Long: 43688\n",
      "  Buy: 17541\n",
      "  Sell: 17541\n",
      "  Short > Long: 2040\n",
      "  Long > Short: 2020\n"
     ]
    }
   ],
   "source": [
    "# Quick stats\n",
    "print(f\"Unique users: {len(set(f['user'] for f in fills))}\")\n",
    "print(f\"Unique coins: {len(set(f['coin'] for f in fills))}\")\n",
    "print(f\"\\nDirections:\")\n",
    "for d, c in Counter(f['dir'] for f in fills).most_common():\n",
    "    print(f\"  {d}: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exploring `explorer_blocks` (Raw Historical Data)\n",
    "\n",
    "For Feb 2023 - Jul 2025, only raw blocks exist. Must reconstruct fills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early block files:\n",
      "  explorer_blocks/0/0/1000.rmp.lz4\n",
      "  explorer_blocks/0/0/10000.rmp.lz4\n",
      "  explorer_blocks/0/0/100000.rmp.lz4\n",
      "  explorer_blocks/0/0/10100.rmp.lz4\n",
      "  explorer_blocks/0/0/10200.rmp.lz4\n"
     ]
    }
   ],
   "source": [
    "# Get earliest block file\n",
    "block_files = list_files('hl-mainnet-node-data', 'explorer_blocks/0/0/', limit=5)\n",
    "print(\"Early block files:\")\n",
    "for f in block_files:\n",
    "    print(f\"  {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 blocks\n",
      "Block range: 901 - 1000\n",
      "Time: 2023-02-26T17:41:39.942659\n"
     ]
    }
   ],
   "source": [
    "# Download and parse\n",
    "if block_files:\n",
    "    data = download('hl-mainnet-node-data', block_files[0])\n",
    "    blocks = parse_msgpack_lz4(data)\n",
    "    print(f\"Loaded {len(blocks)} blocks\")\n",
    "    print(f\"Block range: {blocks[0]['header']['height']} - {blocks[-1]['header']['height']}\")\n",
    "    print(f\"Time: {blocks[0]['header']['block_time']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action types:\n",
      "  order: 100\n",
      "  cancel: 25\n",
      "  SetGlobalAction: 5\n",
      "  CreditBridgeDepositAction: 1\n",
      "  connect: 1\n"
     ]
    }
   ],
   "source": [
    "# Action types in blocks\n",
    "if block_files:\n",
    "    actions = Counter()\n",
    "    for block in blocks:\n",
    "        for tx in block.get('txs', []):\n",
    "            for action in tx.get('actions', []):\n",
    "                actions[action.get('type', 'unknown')] += 1\n",
    "    \n",
    "    print(\"Action types:\")\n",
    "    for t, c in actions.most_common():\n",
    "        print(f\"  {t}: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's in Raw Blocks\n",
    "\n",
    "- Orders (asset, side, price, size)\n",
    "- Cancels\n",
    "- User addresses\n",
    "\n",
    "### What's Missing\n",
    "\n",
    "- Fills (must reconstruct via matching engine)\n",
    "- PnL (must calculate)\n",
    "- Fees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reconstruction: Theoretical\n",
    "\n",
    "Fills can be reconstructed because matching is deterministic:\n",
    "\n",
    "```\n",
    "Same orders + Same sequence = Same fills\n",
    "```\n",
    "\n",
    "**Challenges:**\n",
    "- ~3-4 TB of data\n",
    "- Must track order book state\n",
    "- Edge cases: liquidations, funding, trigger orders\n",
    "\n",
    "**Effort estimate:**\n",
    "\n",
    "| Approach | Coverage | Time |\n",
    "|----------|----------|------|\n",
    "| `node_fills_by_block` only | Jul 2025+ (100%) | 1 day |\n",
    "| + API backfill (10k limit) | ~95% of fills | 3 days |\n",
    "| + Full reconstruction | 100% | 3-4 weeks |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Our Approach\n",
    "\n",
    "**Use `node_fills_by_block`** (Jul 2025 - Present)\n",
    "\n",
    "- ~5 months of complete fill data\n",
    "- Every trader, every fill, with PnL and fees\n",
    "- No reconstruction needed\n",
    "\n",
    "**What we can compute:**\n",
    "\n",
    "| Metric | Source |\n",
    "|--------|--------|\n",
    "| Realized PnL | `SUM(closedPnl)` |\n",
    "| Volume | `SUM(px * sz)` |\n",
    "| Trade count | `COUNT(*)` |\n",
    "| Maker % | `SUM(crossed=false) / COUNT(*)` |\n",
    "| Win rate | Closes with positive PnL / total closes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cost Estimates\n",
    "\n",
    "S3 transfer at $0.09/GB:\n",
    "\n",
    "| Dataset | Size (est.) | Cost |\n",
    "|---------|-------------|------|\n",
    "| `node_fills_by_block` | ~200-400 GB | ~$30 |\n",
    "| `explorer_blocks` | ~3-4 TB | ~$315 |\n",
    "\n",
    "**Start with `node_fills_by_block`.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
