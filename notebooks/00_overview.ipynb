{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperliquid Data Overview\n",
    "\n",
    "This notebook provides a complete overview of Hyperliquid's publicly available data infrastructure.\n",
    "\n",
    "**Goal**: Build a trading engine that deeply understands Hyperliquid through its traders.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Create `.env` from `.env.example`:\n",
    "```\n",
    "AWS_ACCESS_KEY_ID=...\n",
    "AWS_SECRET_ACCESS_KEY=...\n",
    "AWS_REGION=us-east-1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "from vigil import (\n",
    "    download,\n",
    "    get_s3_client,\n",
    "    list_files,\n",
    "    list_prefixes,\n",
    "    parse_jsonl_lz4,\n",
    "    parse_msgpack_lz4,\n",
    ")\n",
    "\n",
    "s3 = get_s3_client()\n",
    "\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Two S3 Buckets\n",
    "\n",
    "From [Hyperliquid docs](https://hyperliquid.gitbook.io/hyperliquid-docs/historical-data):\n",
    "\n",
    "| Bucket | Purpose | Data Types |\n",
    "|--------|---------|------------|\n",
    "| `hyperliquid-archive` | Market data archives | Market data |\n",
    "| `hl-mainnet-node-data` | Node-streamed data | Explorer blocks, trades, fills |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hyperliquid-archive ===\n",
      "  Testnet/\n",
      "  asset_ctxs/\n",
      "  market_data/\n",
      "\n",
      "=== hl-mainnet-node-data ===\n",
      "  explorer_blocks/\n",
      "  misc_events_by_block/\n",
      "  node_fills/\n",
      "  node_fills_by_block/\n",
      "  node_trades/\n",
      "  replica_cmds/\n"
     ]
    }
   ],
   "source": [
    "print(\"=== hyperliquid-archive ===\")\n",
    "for p in list_prefixes('hyperliquid-archive'):\n",
    "    print(f\"  {p}\")\n",
    "\n",
    "print(\"\\n=== hl-mainnet-node-data ===\")\n",
    "for p in list_prefixes('hl-mainnet-node-data'):\n",
    "    print(f\"  {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bucket 1: `hyperliquid-archive`\n",
    "\n",
    "Market data archives. Not the focus for trader analysis, but useful for orderbook studies.\n",
    "\n",
    "| Dataset | Date Range | Content | Format |\n",
    "|---------|------------|---------|--------|\n",
    "| `market_data` | Apr 2023 - Present | L2 orderbook snapshots per coin/hour | JSON+LZ4 |\n",
    "| `asset_ctxs` | May 2023 - Present | Daily asset context (funding, OI, etc.) | CSV+LZ4 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Dataset              First           Last              Days\n",
      "======================================================================\n",
      "market_data          Apr 15, 2023    Nov 02, 2025       925\n",
      "asset_ctxs           May 20, 2023    Nov 02, 2025       898\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Discover date ranges for hyperliquid-archive\n",
    "def fmt_date(d):\n",
    "    return datetime.strptime(d, \"%Y%m%d\").strftime(\"%b %d, %Y\")\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Dataset':<20} {'First':<15} {'Last':<15} {'Days':>6}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# market_data (organized by date folders)\n",
    "dates = list_prefixes(\"hyperliquid-archive\", \"market_data/\")\n",
    "if dates:\n",
    "    first = dates[0].split(\"/\")[-2]\n",
    "    last = dates[-1].split(\"/\")[-2]\n",
    "    print(f\"{'market_data':<20} {fmt_date(first):<15} {fmt_date(last):<15} {len(dates):>6}\")\n",
    "\n",
    "# asset_ctxs (files named by date)\n",
    "files = list_files(\"hyperliquid-archive\", \"asset_ctxs/\", limit=1000)\n",
    "if files:\n",
    "    # list_files returns (key, size) tuples - extract keys\n",
    "    file_dates = [key.split(\"/\")[-1].split(\".\")[0] for key, size in files]\n",
    "    first = file_dates[0]\n",
    "    last = file_dates[-1]\n",
    "    print(f\"{'asset_ctxs':<20} {fmt_date(first):<15} {fmt_date(last):<15} {len(files):>6}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bucket 2: `hl-mainnet-node-data`\n",
    "\n",
    "Node-streamed data. This is what we need for trader analysis.\n",
    "\n",
    "| Dataset | Date Range | Content | Format |\n",
    "|---------|------------|---------|--------|\n",
    "| `node_fills_by_block` | **Jul 2025 - Present** | Fills with PnL, fees, maker/taker | JSON+LZ4 |\n",
    "| `node_fills` | May 2025 - Jul 2025 | Fills (legacy format) | JSON+LZ4 |\n",
    "| `node_trades` | Mar 2025 - Jun 2025 | Trades with buyer/seller | JSON+LZ4 |\n",
    "| `replica_cmds` | Jul 2025 - Present | L1 transactions | JSON+LZ4 |\n",
    "| `misc_events_by_block` | Jul 2025 - Present | Liquidations, funding, etc. | JSON+LZ4 |\n",
    "| `explorer_blocks` | Feb 2023 - Present | Raw blocks (orders, cancels) | MessagePack+LZ4 |\n",
    "\n",
    "**Best dataset**: `node_fills_by_block` â€” complete fill data, no reconstruction needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Dataset                   First           Last              Days\n",
      "======================================================================\n",
      "Fills by block (BEST)     Jul 27, 2025    Nov 30, 2025       127\n",
      "Node fills (legacy)       May 25, 2025    Jul 27, 2025        64\n",
      "Node trades (legacy)      Mar 22, 2025    Jun 21, 2025        66\n",
      "Misc events               Sep 27, 2025    Nov 30, 2025        65\n",
      "Replica commands          2025-01-12      2025-11-29          52\n",
      "Explorer blocks           Block 0         Block 800000000+    N/A\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Discover all date ranges from S3\n",
    "def get_date_range(prefix):\n",
    "    \"\"\"Get first/last date for an hourly prefix\"\"\"\n",
    "    dates = list_prefixes(\"hl-mainnet-node-data\", f\"{prefix}hourly/\")\n",
    "    if not dates:\n",
    "        return None, None, 0\n",
    "    first = dates[0].rstrip(\"/\").split(\"/\")[-1]\n",
    "    last = dates[-1].rstrip(\"/\").split(\"/\")[-1]\n",
    "    return first, last, len(dates)\n",
    "\n",
    "\n",
    "def fmt_date(d):\n",
    "    return datetime.strptime(d, \"%Y%m%d\").strftime(\"%b %d, %Y\")\n",
    "\n",
    "\n",
    "datasets = [\n",
    "    (\"node_fills_by_block/\", \"Fills by block (BEST)\"),\n",
    "    (\"node_fills/\", \"Node fills (legacy)\"),\n",
    "    (\"node_trades/\", \"Node trades (legacy)\"),\n",
    "    (\"misc_events_by_block/\", \"Misc events\"),\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Dataset':<25} {'First':<15} {'Last':<15} {'Days':>6}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for prefix, label in datasets:\n",
    "    first, last, count = get_date_range(prefix)\n",
    "    if first:\n",
    "        print(f\"{label:<25} {fmt_date(first):<15} {fmt_date(last):<15} {count:>6}\")\n",
    "    else:\n",
    "        print(f\"{label:<25} {'N/A':<15} {'N/A':<15} {0:>6}\")\n",
    "\n",
    "# replica_cmds - different structure (ISO timestamp folders, not hourly/)\n",
    "replica_prefixes = list_prefixes(\"hl-mainnet-node-data\", \"replica_cmds/\")\n",
    "if replica_prefixes:\n",
    "    first_ts = replica_prefixes[0].split(\"/\")[1][:10]  # Extract date part\n",
    "    last_ts = replica_prefixes[-1].split(\"/\")[1][:10]\n",
    "    print(f\"{'Replica commands':<25} {first_ts:<15} {last_ts:<15} {len(replica_prefixes):>6}\")\n",
    "\n",
    "# Explorer blocks - different structure (by block number, not date)\n",
    "explorer_prefixes = list_prefixes(\"hl-mainnet-node-data\", \"explorer_blocks/\")\n",
    "if explorer_prefixes:\n",
    "    last_range = explorer_prefixes[-1].split(\"/\")[1]\n",
    "    print(f\"{'Explorer blocks':<25} {'Block 0':<15} {f'Block {last_range}+':<15} {'N/A':>6}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exploring `node_fills_by_block` (The Best Data)\n",
    "\n",
    "Every fill since July 2025 with complete information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using date: 20251129\n",
      "Files for this date: 24\n",
      "Sample files: ['0.lz4', '1.lz4', '10.lz4', '11.lz4', '12.lz4']\n"
     ]
    }
   ],
   "source": [
    "# Find a date with data\n",
    "dates = list_prefixes('hl-mainnet-node-data', 'node_fills_by_block/hourly/')\n",
    "sample_date = dates[-2].rstrip('/').split('/')[-1]  # Second to last (likely complete)\n",
    "print(f\"Using date: {sample_date}\")\n",
    "\n",
    "# List files for this date (files are named {hour}.lz4, not in hour subfolders)\n",
    "files = list_files('hl-mainnet-node-data', f'node_fills_by_block/hourly/{sample_date}/')\n",
    "print(f\"Files for this date: {len(files)}\")\n",
    "print(\"Sample files:\", [k.split('/')[-1] for k, v in files[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 44,641 blocks from hour 12\n",
      "Extracted 170,978 fills\n",
      "\n",
      "Sample fill:\n",
      "{\n",
      "  \"coin\": \"WLFI\",\n",
      "  \"px\": \"0.15995\",\n",
      "  \"sz\": \"77.0\",\n",
      "  \"side\": \"B\",\n",
      "  \"time\": 1764417600000,\n",
      "  \"startPosition\": \"-22206.0\",\n",
      "  \"dir\": \"Close Short\",\n",
      "  \"closedPnl\": \"0.00462\",\n",
      "  \"hash\": \"0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "  \"oid\": 252520932522,\n",
      "  \"crossed\": false,\n",
      "  \"fee\": \"0.0\",\n",
      "  \"tid\": 95174656478359,\n",
      "  \"cloid\": \"0x20251129000000000000000000187575\",\n",
      "  \"feeToken\": \"USDC\",\n",
      "  \"twapId\": null,\n",
      "  \"user\": \"0x4264b5a132e4f263d6de2e0d01512a99ea21ec6e\",\n",
      "  \"block_time\": \"2025-11-29T12:00:00.000242049\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Download and parse hour 12 (midday, typically active)\n",
    "sample_key = f'node_fills_by_block/hourly/{sample_date}/12.lz4'\n",
    "data = download('hl-mainnet-node-data', sample_key)\n",
    "\n",
    "# Each line is a block with multiple fill events\n",
    "blocks = list(parse_jsonl_lz4(data))\n",
    "print(f\"Loaded {len(blocks):,} blocks from hour 12\")\n",
    "\n",
    "# Flatten to individual fills: each event is [user_address, fill_data]\n",
    "fills = []\n",
    "for block in blocks:\n",
    "    for user, fill_data in block.get('events', []):\n",
    "        fill_data['user'] = user\n",
    "        fill_data['block_time'] = block['block_time']\n",
    "        fills.append(fill_data)\n",
    "\n",
    "print(f\"Extracted {len(fills):,} fills\\n\")\n",
    "print(\"Sample fill:\")\n",
    "print(json.dumps(fills[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structure\n",
    "\n",
    "Each file contains **blocks** (one per line), each block contains multiple **fill events**:\n",
    "\n",
    "```\n",
    "{block_time, block_number, events: [[user_address, fill_data], ...]}\n",
    "```\n",
    "\n",
    "### Fill Schema\n",
    "\n",
    "| Field | Description |\n",
    "|-------|-------------|\n",
    "| `user` | Wallet address (from event tuple) |\n",
    "| `coin` | Asset traded |\n",
    "| `px`, `sz` | Price and size |\n",
    "| `side` | B (buy) or A (ask/sell) |\n",
    "| `dir` | Direction: Open Long, Open Short, Close Long, Close Short, Long > Short, Short > Long |\n",
    "| `closedPnl` | Realized PnL (only on closes) |\n",
    "| `fee` | Fee paid (negative = rebate) |\n",
    "| `crossed` | true = taker, false = maker |\n",
    "| `startPosition` | Position before this fill |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique users: 5767\n",
      "Unique coins: 239\n",
      "\n",
      "Directions:\n",
      "  Open Long: 39744\n",
      "  Close Long: 37126\n",
      "  Open Short: 36759\n",
      "  Close Short: 34161\n",
      "  Buy: 10371\n",
      "  Sell: 10371\n",
      "  Long > Short: 1233\n",
      "  Short > Long: 1213\n"
     ]
    }
   ],
   "source": [
    "# Quick stats\n",
    "print(f\"Unique users: {len(set(f['user'] for f in fills))}\")\n",
    "print(f\"Unique coins: {len(set(f['coin'] for f in fills))}\")\n",
    "print(f\"\\nDirections:\")\n",
    "for d, c in Counter(f['dir'] for f in fills).most_common():\n",
    "    print(f\"  {d}: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exploring `explorer_blocks` (Raw Historical Data)\n",
    "\n",
    "For Feb 2023 - Jul 2025, only raw blocks exist. Must reconstruct fills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early block files:\n",
      "  explorer_blocks/0/0/1000.rmp.lz4\n",
      "  explorer_blocks/0/0/10000.rmp.lz4\n",
      "  explorer_blocks/0/0/100000.rmp.lz4\n",
      "  explorer_blocks/0/0/10100.rmp.lz4\n",
      "  explorer_blocks/0/0/10200.rmp.lz4\n"
     ]
    }
   ],
   "source": [
    "# Get earliest block file\n",
    "block_files = list_files(\"hl-mainnet-node-data\", \"explorer_blocks/0/0/\", limit=5)\n",
    "print(\"Early block files:\")\n",
    "for key, size in block_files:\n",
    "    print(f\"  {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 blocks\n",
      "Block range: 901 - 1000\n",
      "Time: 2023-02-26T17:41:39.942659\n"
     ]
    }
   ],
   "source": [
    "# Download and parse\n",
    "if block_files:\n",
    "    key, size = block_files[0]\n",
    "    data = download(\"hl-mainnet-node-data\", key)\n",
    "    blocks = parse_msgpack_lz4(data)\n",
    "    print(f\"Loaded {len(blocks)} blocks\")\n",
    "    print(f\"Block range: {blocks[0]['header']['height']} - {blocks[-1]['header']['height']}\")\n",
    "    print(f\"Time: {blocks[0]['header']['block_time']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action types:\n",
      "  order: 100\n",
      "  cancel: 25\n",
      "  SetGlobalAction: 5\n",
      "  CreditBridgeDepositAction: 1\n",
      "  connect: 1\n"
     ]
    }
   ],
   "source": [
    "# Action types in blocks\n",
    "if block_files:\n",
    "    actions = Counter()\n",
    "    for block in blocks:\n",
    "        for tx in block.get('txs', []):\n",
    "            for action in tx.get('actions', []):\n",
    "                actions[action.get('type', 'unknown')] += 1\n",
    "    \n",
    "    print(\"Action types:\")\n",
    "    for t, c in actions.most_common():\n",
    "        print(f\"  {t}: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's in Raw Blocks\n",
    "\n",
    "- Orders (asset, side, price, size)\n",
    "- Cancels\n",
    "- User addresses\n",
    "\n",
    "### What's Missing\n",
    "\n",
    "- Fills\n",
    "- PnL\n",
    "- Fees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Our Approach\n",
    "\n",
    "**Use `node_fills_by_block`** (Jul 2025 - Present)\n",
    "\n",
    "- ~5 months of complete fill data\n",
    "- Every trader, every fill, with PnL and fees\n",
    "- Emphasize trader analysis\n",
    "- No reconstruction needed\n",
    "\n",
    "\n",
    "## Reconstruction: Theoretical\n",
    "\n",
    "To get pre-July 2025 fills, we can reconstruct because matching is deterministic. Same orders + same sequence = same fills.\n",
    "\n",
    "**Challenges:**\n",
    "- Block explorer data is huge\n",
    "- Must track order book state\n",
    "- Edge cases: liquidations, funding, trigger orders (would use misc_events_by_block)\n",
    "- Would need to rewrite the entire matching engine (their node is closed source docker binary)\n",
    "\n",
    "**Alternative:**\n",
    "- Can get two months of fill data from `node_fills` (legacy format)\n",
    "- Best effort and fetch latest 10k trades. Reconcile with other data providers that may have more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
