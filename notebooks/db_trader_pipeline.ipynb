{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trader Analysis Pipeline (Database)\n",
    "\n",
    "Build the trader profiling pipeline connected to TimescaleDB.\n",
    "\n",
    "## Setup\n",
    "1. Run `just sample-data` to pull sample fills from cloud DB\n",
    "2. Run `just db-local-up` to start local TimescaleDB\n",
    "3. Run `just load-sample-local` to load sample into local DB\n",
    "4. Run this notebook\n",
    "\n",
    "## Tiers\n",
    "1. **Tier 1**: Direct aggregations (volume, PnL, fees, maker%)\n",
    "2. **Tier 2**: Position reconstruction (holding periods, win rate per trade)\n",
    "3. **Tier 3**: Performance metrics (Sharpe, ROI, MTM/TV)\n",
    "4. **Tier 4**: Classification (HFT, Smart Directional, Basis, Retail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from vigil.db import execute_query, get_db_connection\n",
    "\n",
    "# Test connection\n",
    "conn = get_db_connection()\n",
    "result = execute_query(\"SELECT COUNT(*) as cnt FROM fills\", conn)\n",
    "print(f\"Connected! Fills in database: {result['cnt'][0]:,}\")\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample of fills for analysis\n",
    "conn = get_db_connection()\n",
    "\n",
    "fills = execute_query(\"\"\"\n",
    "    SELECT\n",
    "        time, user_address, coin, px, sz, side, dir,\n",
    "        start_position, closed_pnl, fee, crossed,\n",
    "        twap_id, builder, liquidation::TEXT as liquidation\n",
    "    FROM fills\n",
    "    LIMIT 500000\n",
    "\"\"\", conn)\n",
    "\n",
    "print(f\"Loaded {len(fills):,} fills\")\n",
    "print(f\"Columns: {fills.columns}\")\n",
    "fills.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string columns to numeric for calculations\n",
    "fills = fills.with_columns([\n",
    "    pl.col(\"px\").cast(pl.Float64).alias(\"price\"),\n",
    "    pl.col(\"sz\").cast(pl.Float64).alias(\"size\"),\n",
    "    pl.col(\"closed_pnl\").cast(pl.Float64).fill_null(0).alias(\"pnl\"),\n",
    "    pl.col(\"fee\").cast(pl.Float64).fill_null(0).alias(\"fee_num\"),\n",
    "    pl.col(\"start_position\").cast(pl.Float64).fill_null(0).alias(\"start_pos\"),\n",
    "])\n",
    "\n",
    "# Calculate notional value\n",
    "fills = fills.with_columns(\n",
    "    (pl.col(\"price\") * pl.col(\"size\")).alias(\"notional\")\n",
    ")\n",
    "\n",
    "# Check liquidation data\n",
    "print(f\"Liquidation fills: {(fills['liquidation'].is_not_null()).sum()}\")\n",
    "print(f\"TWAP fills: {(fills['twap_id'].is_not_null()).sum()}\")\n",
    "print(f\"Builder fills: {(fills['builder'].is_not_null()).sum()}\")\n",
    "\n",
    "fills.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tier 1: Direct Aggregations\n",
    "\n",
    "These can become continuous aggregates in TimescaleDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trader lifetime stats\n",
    "trader_stats = fills.group_by(\"user_address\").agg([\n",
    "    # Activity\n",
    "    pl.len().alias(\"total_trades\"),\n",
    "    pl.col(\"notional\").sum().alias(\"total_volume\"),\n",
    "    pl.col(\"coin\").n_unique().alias(\"unique_coins\"),\n",
    "    pl.col(\"time\").min().alias(\"first_trade\"),\n",
    "    pl.col(\"time\").max().alias(\"last_trade\"),\n",
    "    \n",
    "    # PnL\n",
    "    pl.col(\"pnl\").sum().alias(\"realized_pnl\"),\n",
    "    pl.col(\"fee_num\").sum().alias(\"fees_paid\"),\n",
    "    \n",
    "    # Maker/Taker\n",
    "    pl.col(\"crossed\").mean().alias(\"taker_pct\"),\n",
    "    \n",
    "    # Win rate\n",
    "    (pl.col(\"pnl\") > 0).sum().alias(\"winning_fills\"),\n",
    "    (pl.col(\"pnl\") < 0).sum().alias(\"losing_fills\"),\n",
    "    (pl.col(\"pnl\") != 0).sum().alias(\"closing_fills\"),\n",
    "    \n",
    "    # Liquidations\n",
    "    pl.col(\"liquidation\").is_not_null().sum().alias(\"liquidation_fills\"),\n",
    "    \n",
    "    # TWAP\n",
    "    pl.col(\"twap_id\").is_not_null().sum().alias(\"twap_fills\"),\n",
    "])\n",
    "\n",
    "# Derived metrics\n",
    "trader_stats = trader_stats.with_columns([\n",
    "    (1 - pl.col(\"taker_pct\")).alias(\"maker_pct\"),\n",
    "    (pl.col(\"realized_pnl\") - pl.col(\"fees_paid\")).alias(\"net_pnl\"),\n",
    "    (pl.col(\"winning_fills\") / pl.col(\"closing_fills\").cast(pl.Float64)).fill_null(0).alias(\"win_rate\"),\n",
    "    (pl.col(\"realized_pnl\") / pl.col(\"total_volume\")).alias(\"mtm_tv\"),\n",
    "    (pl.col(\"liquidation_fills\") / pl.col(\"total_trades\").cast(pl.Float64)).fill_null(0).alias(\"liquidation_pct\"),\n",
    "])\n",
    "\n",
    "print(f\"Computed stats for {len(trader_stats):,} traders\")\n",
    "trader_stats.sort(\"total_volume\", descending=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily aggregation - for Sharpe ratio calculation\n",
    "MS_PER_DAY = 86400000\n",
    "\n",
    "daily_stats = fills.with_columns(\n",
    "    (pl.col(\"time\") // MS_PER_DAY * MS_PER_DAY).alias(\"day\")\n",
    ").group_by([\"user_address\", \"day\"]).agg([\n",
    "    pl.len().alias(\"trades\"),\n",
    "    pl.col(\"notional\").sum().alias(\"volume\"),\n",
    "    pl.col(\"pnl\").sum().alias(\"daily_pnl\"),\n",
    "    pl.col(\"fee_num\").sum().alias(\"daily_fees\"),\n",
    "])\n",
    "\n",
    "print(f\"Daily stats: {len(daily_stats):,} trader-days\")\n",
    "daily_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tier 2: Position Reconstruction\n",
    "\n",
    "Track openâ†’close cycles to compute holding periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the 'dir' column values\n",
    "fills.group_by(\"dir\").agg(pl.len().alias(\"count\")).sort(\"count\", descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_positions(fills_df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Track open->close cycles for each (user, coin) pair.\n",
    "    Returns completed trades with holding period and PnL.\n",
    "    \"\"\"\n",
    "    fills = fills_df.sort([\"user_address\", \"coin\", \"time\"])\n",
    "    \n",
    "    positions = {}\n",
    "    trades = []\n",
    "    \n",
    "    for row in fills.iter_rows(named=True):\n",
    "        key = (row[\"user_address\"], row[\"coin\"])\n",
    "        direction = row[\"dir\"]\n",
    "        \n",
    "        if not direction:\n",
    "            continue\n",
    "            \n",
    "        if direction.startswith(\"Open\"):\n",
    "            if key not in positions or positions[key][\"size\"] == 0:\n",
    "                positions[key] = {\n",
    "                    \"entry_time\": row[\"time\"],\n",
    "                    \"size\": row[\"size\"],\n",
    "                    \"side\": \"long\" if \"Long\" in direction else \"short\",\n",
    "                }\n",
    "            else:\n",
    "                positions[key][\"size\"] += row[\"size\"]\n",
    "                \n",
    "        elif direction.startswith(\"Close\"):\n",
    "            if key in positions and positions[key][\"size\"] > 0:\n",
    "                pos = positions[key]\n",
    "                holding_ms = row[\"time\"] - pos[\"entry_time\"]\n",
    "                \n",
    "                trades.append({\n",
    "                    \"user_address\": row[\"user_address\"],\n",
    "                    \"coin\": row[\"coin\"],\n",
    "                    \"side\": pos[\"side\"],\n",
    "                    \"holding_period_ms\": holding_ms,\n",
    "                    \"holding_period_hours\": holding_ms / 3600000,\n",
    "                    \"closed_pnl\": row[\"pnl\"],\n",
    "                    \"exit_time\": row[\"time\"],\n",
    "                })\n",
    "                \n",
    "                pos[\"size\"] -= row[\"size\"]\n",
    "                if pos[\"size\"] <= 0:\n",
    "                    positions[key] = {\"size\": 0, \"entry_time\": None, \"side\": None}\n",
    "    \n",
    "    return pl.DataFrame(trades)\n",
    "\n",
    "# Run on top 100 traders by volume (faster than all)\n",
    "top_traders = trader_stats.sort(\"total_volume\", descending=True).head(100)[\"user_address\"].to_list()\n",
    "fills_subset = fills.filter(pl.col(\"user_address\").is_in(top_traders))\n",
    "print(f\"Processing {len(fills_subset):,} fills for top 100 traders...\")\n",
    "\n",
    "trades = reconstruct_positions(fills_subset)\n",
    "print(f\"Found {len(trades):,} completed trades\")\n",
    "trades.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holding period stats per trader\n",
    "if len(trades) > 0:\n",
    "    holding_stats = trades.group_by(\"user_address\").agg([\n",
    "        pl.len().alias(\"completed_trades\"),\n",
    "        pl.col(\"holding_period_hours\").mean().alias(\"avg_hold_hours\"),\n",
    "        pl.col(\"holding_period_hours\").median().alias(\"median_hold_hours\"),\n",
    "        pl.col(\"closed_pnl\").sum().alias(\"total_pnl\"),\n",
    "        (pl.col(\"closed_pnl\") > 0).mean().alias(\"trade_win_rate\"),\n",
    "    ])\n",
    "    \n",
    "    print(\"Holding period distribution:\")\n",
    "    print(trades[\"holding_period_hours\"].describe())\n",
    "    print()\n",
    "    holding_stats.sort(\"completed_trades\", descending=True).head(20)\n",
    "else:\n",
    "    print(\"No completed trades found in sample\")\n",
    "    holding_stats = pl.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tier 3: Performance Metrics\n",
    "\n",
    "Sharpe ratio, turnover, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sharpe(daily_pnl: np.ndarray, risk_free: float = 0) -> float:\n",
    "    \"\"\"Annualized Sharpe ratio from daily PnL series.\"\"\"\n",
    "    if len(daily_pnl) < 2:\n",
    "        return 0.0\n",
    "    excess = daily_pnl - risk_free\n",
    "    std = excess.std()\n",
    "    if std == 0:\n",
    "        return 0.0\n",
    "    return float((excess.mean() / std) * np.sqrt(365))\n",
    "\n",
    "\n",
    "# Calculate Sharpe for each trader with enough data\n",
    "sharpe_results = []\n",
    "\n",
    "for user in daily_stats[\"user_address\"].unique().to_list():\n",
    "    user_daily = daily_stats.filter(pl.col(\"user_address\") == user).sort(\"day\")\n",
    "    if len(user_daily) >= 2:\n",
    "        daily_pnl = user_daily[\"daily_pnl\"].to_numpy()\n",
    "        sharpe = calculate_sharpe(daily_pnl)\n",
    "        sharpe_results.append({\n",
    "            \"user_address\": user,\n",
    "            \"sharpe_ratio\": sharpe,\n",
    "            \"trading_days\": len(user_daily),\n",
    "        })\n",
    "\n",
    "sharpe_df = pl.DataFrame(sharpe_results)\n",
    "print(f\"Calculated Sharpe for {len(sharpe_df):,} traders\")\n",
    "sharpe_df.sort(\"sharpe_ratio\", descending=True).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tier 4: Classification\n",
    "\n",
    "Combine all metrics and classify traders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all metrics\n",
    "profiles = trader_stats\n",
    "\n",
    "# Add Sharpe\n",
    "if len(sharpe_df) > 0:\n",
    "    profiles = profiles.join(sharpe_df, on=\"user_address\", how=\"left\")\n",
    "\n",
    "# Add holding period stats\n",
    "if len(trades) > 0 and len(holding_stats) > 0:\n",
    "    profiles = profiles.join(holding_stats.select([\"user_address\", \"avg_hold_hours\"]), on=\"user_address\", how=\"left\")\n",
    "\n",
    "profiles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_trader(row: dict) -> str:\n",
    "    \"\"\"\n",
    "    Classify trader based on behavioral metrics.\n",
    "    \"\"\"\n",
    "    maker_pct = row.get(\"maker_pct\", 0) or 0\n",
    "    mtm_tv = row.get(\"mtm_tv\", 0) or 0\n",
    "    avg_hold = row.get(\"avg_hold_hours\", 999) or 999\n",
    "    net_pnl = row.get(\"net_pnl\", 0) or 0\n",
    "    sharpe = row.get(\"sharpe_ratio\", 0) or 0\n",
    "    liquidation_pct = row.get(\"liquidation_pct\", 0) or 0\n",
    "    \n",
    "    # LIQUIDATOR: Primarily liquidates others\n",
    "    if liquidation_pct >= 0.20:\n",
    "        return \"LIQUIDATOR\"\n",
    "    \n",
    "    # HFT: High maker%, low edge per trade\n",
    "    if (maker_pct >= 0.70 and abs(mtm_tv) <= 0.001):\n",
    "        return \"HFT\"\n",
    "    \n",
    "    # Smart Directional: High PnL, good risk-adjusted returns\n",
    "    if (net_pnl >= 10000 and mtm_tv >= 0.001 and sharpe >= 1.0):\n",
    "        return \"SMART_DIRECTIONAL\"\n",
    "    \n",
    "    # Basis: Long holds\n",
    "    if avg_hold >= 24:\n",
    "        return \"BASIS\"\n",
    "    \n",
    "    return \"RETAIL\"\n",
    "\n",
    "\n",
    "# Classify each trader\n",
    "classifications = [classify_trader(row) for row in profiles.iter_rows(named=True)]\n",
    "profiles = profiles.with_columns(pl.Series(\"trader_type\", classifications))\n",
    "\n",
    "# Distribution\n",
    "print(\"Trader type distribution:\")\n",
    "profiles.group_by(\"trader_type\").agg([\n",
    "    pl.len().alias(\"count\"),\n",
    "    pl.col(\"total_volume\").sum().alias(\"total_volume\"),\n",
    "    pl.col(\"net_pnl\").sum().alias(\"total_pnl\"),\n",
    "    pl.col(\"liquidation_fills\").sum().alias(\"liquidation_fills\"),\n",
    "]).sort(\"count\", descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View top traders by type\n",
    "for trader_type in [\"HFT\", \"SMART_DIRECTIONAL\", \"LIQUIDATOR\", \"BASIS\"]:\n",
    "    print(f\"\\n=== {trader_type} ===\")\n",
    "    subset = profiles.filter(pl.col(\"trader_type\") == trader_type)\n",
    "    if len(subset) > 0:\n",
    "        display_cols = [\"user_address\", \"total_trades\", \"total_volume\", \"net_pnl\", \n",
    "                       \"maker_pct\", \"mtm_tv\", \"win_rate\", \"liquidation_pct\"]\n",
    "        display_cols = [c for c in display_cols if c in subset.columns]\n",
    "        print(subset.select(display_cols).sort(\"total_volume\", descending=True).head(5))\n",
    "    else:\n",
    "        print(\"No traders in this category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liquidation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Parse liquidation events\n",
    "liq_fills = fills.filter(pl.col(\"liquidation\").is_not_null())\n",
    "print(f\"Total liquidation fills: {len(liq_fills):,}\")\n",
    "\n",
    "if len(liq_fills) > 0:\n",
    "    # Parse JSON\n",
    "    liq_events = []\n",
    "    for row in liq_fills.iter_rows(named=True):\n",
    "        try:\n",
    "            data = json.loads(row[\"liquidation\"])\n",
    "            liq_events.append({\n",
    "                \"time\": row[\"time\"],\n",
    "                \"coin\": row[\"coin\"],\n",
    "                \"liquidator\": row[\"user_address\"],\n",
    "                \"liquidated_user\": data.get(\"liquidatedUser\"),\n",
    "                \"mark_price\": float(data.get(\"markPx\", 0)),\n",
    "                \"size\": row[\"size\"],\n",
    "                \"notional\": row[\"notional\"],\n",
    "                \"method\": data.get(\"method\"),\n",
    "            })\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    liq_df = pl.DataFrame(liq_events)\n",
    "    print(f\"Parsed {len(liq_df):,} liquidation events\")\n",
    "    \n",
    "    # Top liquidated coins\n",
    "    print(\"\\nTop coins by liquidation volume:\")\n",
    "    print(liq_df.group_by(\"coin\").agg([\n",
    "        pl.len().alias(\"count\"),\n",
    "        pl.col(\"notional\").sum().alias(\"total_notional\"),\n",
    "    ]).sort(\"total_notional\", descending=True).head(10))\n",
    "    \n",
    "    # Top liquidators\n",
    "    print(\"\\nTop liquidators:\")\n",
    "    print(liq_df.group_by(\"liquidator\").agg([\n",
    "        pl.len().alias(\"count\"),\n",
    "        pl.col(\"notional\").sum().alias(\"total_notional\"),\n",
    "    ]).sort(\"total_notional\", descending=True).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the final profile schema\n",
    "print(\"Final trader_profiles schema:\")\n",
    "print(profiles.schema)\n",
    "\n",
    "# Save profiles for reference\n",
    "profiles.write_parquet(\"../data/sample_profiles.parquet\")\n",
    "print(f\"\\nSaved {len(profiles):,} profiles to data/sample_profiles.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Push schema to cloud DB: `just db-migrate`\n",
    "2. Refresh continuous aggregates: `just db-refresh-aggregates`\n",
    "3. Run full analysis: `just analyze`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
